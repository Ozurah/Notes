> <span style="font-size: 1.5em">üìñ</span> <span style="color: orange; font-size: 1.3em;">Pr√©sentation `Apprentissage supervis√© et non`</span>

# Apprentissage supervis√©
On a des donn√©es d'entr√©es et des donn√©es de sortie.
On donne des infos (humain) pour que l'ordinateur puisse apprendre.

Exemple on dit "Sa c'est un chat", "sa non" aux images de bases et l'IA apprend √† reconnaitre un chat.

Le fait que dire que c'est un chat c'est "classer" l'image dans la classe (cat√©gorie) "chat".

Les diff√©rentes classes sont connues √† priori.
- La t√¢che de l‚Äôapprentissage est guid√©e par un superviseur.
- Exemple : Reconnaissance de forme ‚Äì Pour chaque forme, c‚Äôest le
superviseur qui d√©cide (sait !) √† quelle classe elle est attribu√©e

## Surrapprentissage
Eviter le surapprentissage (overfitting)
- L‚Äôalgorithme apprend trop bien les donn√©es d‚Äôentrainement (equivalant √† apprendre par coeur)

Exemple d'overfitting : L'IA a su d√©tecter les erreurs sur l'image qui sont dans la s√©paration jaune (au lieu de bleue)
![](Screen/2023-02-24-13-06-09.png)

Comment d√©tecter l'overfitting ?
- En regardant les donn√©es d'entrainement et de test, on d√©tecte s'il y a diff√©rence entre les deux
- On voit sur l'image qu'il y a overfitting ou il y a le ‚ö†Ô∏è sur l'image
  ![](Screen/2023-02-24-13-07-10.png)

# Apprentissage non supervis√©
On a des donn√©es d'entr√©es mais pas de donn√©es de sortie.

- La t√¢che de base de l'apprentissage non supervis√© est de
trouver une structure cach√©e dans des donn√©es non
√©tiquet√©es

# KNN
- K-Nearest Neighbors


![](Screen/2023-02-24-13-24-05.png)
![](Screen/2023-02-24-13-24-24.png)
(remarque : le carr√© bleu au dessus du cercle vert est plus √©loign√© que ceux reli√©s)
Les formes les plus pr√©sentes sont le r√©sultat de la classification

Pour fixer la valeur de K, on cherche quel valeur de K donne le meilleur r√©sultat (plus gros taux de r√©ussite) sur les donn√©es de test.



# MSE
- Mean Squared Error

On recherche les valeurs y. On a le $y_pr√©dit$ et le $y_{r√©el}$

MSE ==> $\displaystyle\sum_{i=N} (\frac{(y_{pr√©dit} - y_{r√©el})^2}{N})$

Si on fait aucune erreur, on a MSE = 0.

Il s'agit en quelque sorte de la marges d'erreur.

# Bagging

Algorithme servant √† calculer la moyenne de plusieurs r√©sultats. Exemple :
- Des donn√©es al√©atoires sont prises √† hauteur de 80% de train, 20% de test
- On r√©p√®te l'√©tape pr√©c√©dente plusieurs fois
- On regroupe les r√©sultats de chaque test et on en fait la moyenne