> <span style="font-size: 1.5em">üìñ</span> <span style="color: orange; font-size: 1.3em;">Pr√©sentation `Cours de r√©gression lin√©aire`</span>

Classificiation : Y est discret (ensembles fini) (pr√©dire une classe; cluster, ex. la couleur)
Regression : Y est continue (pr√©dire une valeur continue, ex. l'√¢ge)


y = ce qu'on cherche
$\hat{y}$ = ce qu'on a pr√©dit

y - $\hat{y}$ = erreur

# M√©thode de la descente de gradient

Le point de d√©part est arbitraire

On doit d√©finir le pas avec le quel on va avanc√©
Et calculer le gradient en un point

![](Screen/2023-03-10-13-21-20.png)
On risque de tomber dans un minimum local, il peut donc √™tre int√©ressant de faire plusieurs descentes de gradient avec des points de d√©part diff√©rents.


## Algorithme

!!! info D√©crit dans les slides 29-34

![](Screen/2023-03-10-13-22-38.png)


![](Screen/2023-03-10-13-23-11.png)

$\alpha$ = pas
La fraction = d√©riv√©e partielle de la fonction de co√ªt par rapport √† $\theta_j$

On peut choisir la valeur de $\alpha$ de mani√®re arbitraire, mais il est pr√©f√©rable de choisir une valeur qui permet de converger rapidement vers le minimum global.
Une petite valeur demande plus de temps de calcul, mais une trop grande pourrait faire que l'on ne converge pas vers le minimum global (ne le trouve pas).