> <span style="font-size: 1.5em">ğŸ“–</span> <span style="color: orange; font-size: 1.3em;">PrÃ©sentation `Clustering`</span>


Algorithmes automatiques pour la classification

Ce n'est pas du machine learning.

# Fonctionnement

On commence avec M classe
`M = nombre de classe`

On agrÃ¨ge les classes les plus proches pour diminuer le nombre de classe

Pour dÃ©terminer les classes, on utilise des distances : Exemple de la distance minimale
![](Screen/2023-03-03-13-18-33.png)
`a`, `b`, `c`, `d` sont des classes (noms)
- Sur le 2eme tableau, on a fusionner les classes a et d. Quand Ã  la valeur `2` de la classe `B`, elle a Ã©tÃ© obtenue :
  $dist(b, C_{a,b}) = min(dist_{a,b}, dist_{b,d})$
- Et on rÃ©pÃ¨te l'opÃ©ration jusqu'Ã  ce qu'il n'y ait plus que N classes (N = nombre de classe souhaitÃ©, ou un seuil de distance)
- Formule gÃ©nÃ©rale :
  $d(C_1, C_2) = min_{i\epsilon C_1 \& j\epsilon C_2}(d_{i,j})$


# Algorithme K-Means
> Explications de l'algo en slide 16-17

![](Screen/2023-03-03-13-50-56.png)


Force :
- maximiser les distances inter-classes
- Minimiser les distances intra-classes
Faiblesse :
- Nombre de classe fixÃ© (k; nombre de cluster connu)
- Point de gravitÃ© est "arbitraire, non rÃ©el"
- Groupes non convexes
  ![](Screen/2023-03-03-13-52-01.png)

# Algorithme K-Medoids

On a plus de points de gravitÃ© (comparÃ© Ã  avant), mais un reprÃ©sentant par classes

> Image : On dÃ©fini un dÃ©lÃ©guÃ© pour la classe

Faiblesse :
- Calcul toutes les possibilitÃ©s pour trouver le meilleur reprÃ©sentant de chaque classe -> Couteux en temps de calcul

# KMeans vs KMedoids
> 1 = KMeans, 2 = KMedoids
> croix = centre de gravitÃ©
> Ã©toile = "dÃ©lÃ©guÃ©"

![](Screen/2023-03-03-14-15-32.png)